import os
from pathlib import Path
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import timm
from torch.cuda.amp import autocast, GradScaler
import subprocess

# =======================
# 1️⃣ GPU / CPU selection
# =======================
def pick_free_gpu():
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,noheader,nounits'],
            stdout=subprocess.PIPE
        )
        free_mem = [int(x) for x in result.stdout.decode('utf-8').split('\n') if x.strip()]
        best_gpu = free_mem.index(max(free_mem))
        return best_gpu
    except Exception:
        return None

gpu_id = pick_free_gpu()
if gpu_id is not None:
    device = torch.device(f"cuda:{gpu_id}")
    print(f"Using GPU {gpu_id}")
else:
    device = torch.device("cpu")
    print("No GPU available, using CPU")

# =======================
# 2️⃣ Dataset class
# =======================
class FracAtlasDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        image = Image.open(self.images[idx]).convert("RGB")
        label = self.labels[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

# =======================
# 3️⃣ Prepare dataset
# =======================
dataset_path = Path("/lapix/FracAtlas/FracAtlas/images")
classes = ["Fractured", "Non_fractured"]

all_images = []
all_labels = []

for label, cls in enumerate(classes):
    cls_dir = dataset_path / cls
    for img_file in cls_dir.glob("*"):
        all_images.append(img_file)
        all_labels.append(label)

train_imgs, val_imgs, train_labels, val_labels = train_test_split(
    all_images, all_labels, test_size=0.2, stratify=all_labels, random_state=42
)

# =======================
# 4️⃣ Transforms
# =======================
transform = transforms.Compose([
    transforms.Resize((384, 384)),  # Change to (224,224) if memory is tight
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])
])

train_dataset = FracAtlasDataset(train_imgs, train_labels, transform=transform)
val_dataset = FracAtlasDataset(val_imgs, val_labels, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Reduce batch_size if needed
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

# =======================
# 5️⃣ Model setup
# =======================
model = timm.create_model('vit_base_patch16_384', pretrained=True)
model.head = nn.Linear(model.head.in_features, 2)  # Binary classification
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)
scaler = GradScaler()  # Mixed precision

# =======================
# 6️⃣ Training loop
# =======================
num_epochs = 10
best_val_acc = 0

for epoch in range(num_epochs):
    model.train()
    train_losses = []
    
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        
        with autocast():  # Mixed precision
            outputs = model(images)
            loss = criterion(outputs, labels)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        train_losses.append(loss.item())
    
    # Validation
    model.eval()
    val_labels_list = []
    val_preds_list = []
    
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            preds = torch.argmax(outputs, dim=1)
            
            val_labels_list.extend(labels.cpu().numpy())
            val_preds_list.extend(preds.cpu().numpy())
    
    val_acc = accuracy_score(val_labels_list, val_preds_list)
    val_prec = precision_score(val_labels_list, val_preds_list)
    val_rec = recall_score(val_labels_list, val_preds_list)
    val_f1 = f1_score(val_labels_list, val_preds_list)
    
    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"Train Loss: {sum(train_losses)/len(train_losses):.4f}")
    print(f"Val Acc: {val_acc:.4f} | Precision: {val_prec:.4f} | Recall: {val_rec:.4f} | F1: {val_f1:.4f}")
    
    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), "vit_fracatlas_best.pth")
        print("✅ Best model saved!")
    
    scheduler.step()
